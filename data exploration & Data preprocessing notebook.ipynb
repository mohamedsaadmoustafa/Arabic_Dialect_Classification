{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Let's start with importing necessary libraries","metadata":{}},{"cell_type":"code","source":"### Problem Solution to show arabic letters from right to left\n! pip install arabic_reshaper\n! pip install python-bidi","metadata":{"execution":{"iopub.status.busy":"2022-03-13T14:48:45.847458Z","iopub.execute_input":"2022-03-13T14:48:45.848259Z","iopub.status.idle":"2022-03-13T14:49:04.068697Z","shell.execute_reply.started":"2022-03-13T14:48:45.848203Z","shell.execute_reply":"2022-03-13T14:49:04.067994Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport requests\nimport json\nimport tensorflow as tf\nimport unicodedata\nimport re\nimport os\n\n# visualization packages\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#sns.color_palette(\"rocket_r\", as_cmap=True)\ncmap = sns.diverging_palette(0, 230, 90, 60, as_cmap=True)\nplt.rcParams.update({'font.size': 22})\nnp.random.seed(42)\n# ignore warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-13T14:49:04.071243Z","iopub.execute_input":"2022-03-13T14:49:04.071590Z","iopub.status.idle":"2022-03-13T14:49:09.342271Z","shell.execute_reply.started":"2022-03-13T14:49:04.071554Z","shell.execute_reply":"2022-03-13T14:49:09.341466Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"> Show available files","metadata":{}},{"cell_type":"code","source":"# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2022-03-13T14:49:09.343496Z","iopub.execute_input":"2022-03-13T14:49:09.343750Z","iopub.status.idle":"2022-03-13T14:49:09.357179Z","shell.execute_reply.started":"2022-03-13T14:49:09.343721Z","shell.execute_reply":"2022-03-13T14:49:09.356345Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"Using `arabic_reshaper` library to reconstruct Arabic sentences to be used in applications that don't support Arabic script.","metadata":{}},{"cell_type":"code","source":"import arabic_reshaper\nfrom bidi.algorithm import get_display\n\nreshaped_text = arabic_reshaper.reshape(u'لغةٌ عربيّة')\nartext = get_display(reshaped_text)\n\nplt.text(0.25, 0.45, artext , name = 'Times New Roman',fontsize=25)","metadata":{"execution":{"iopub.status.busy":"2022-03-13T14:49:09.359827Z","iopub.execute_input":"2022-03-13T14:49:09.360253Z","iopub.status.idle":"2022-03-13T14:49:09.605118Z","shell.execute_reply.started":"2022-03-13T14:49:09.360208Z","shell.execute_reply":"2022-03-13T14:49:09.604350Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"Reading dataset, drop id column that used to btain data texts from [API](https://recruitment.aimtechnologies.co/ai-tasks) then rename columns for our text to tweets. ","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"../input/alldialectdataset/out.csv\", encoding=\"utf-8\", lineterminator='\\n')\ndf.drop(columns=[\"id\"], inplace=True)\ndf.columns = ['dialect', 'tweets']\ndf.tail()","metadata":{"execution":{"iopub.status.busy":"2022-03-13T14:49:09.606397Z","iopub.execute_input":"2022-03-13T14:49:09.606598Z","iopub.status.idle":"2022-03-13T14:49:12.771901Z","shell.execute_reply.started":"2022-03-13T14:49:09.606573Z","shell.execute_reply":"2022-03-13T14:49:12.771152Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"df.describe().T","metadata":{"execution":{"iopub.status.busy":"2022-03-13T14:49:12.773250Z","iopub.execute_input":"2022-03-13T14:49:12.773678Z","iopub.status.idle":"2022-03-13T14:49:13.526070Z","shell.execute_reply.started":"2022-03-13T14:49:12.773616Z","shell.execute_reply":"2022-03-13T14:49:13.525233Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"`dialect` column has 18 unique class and 'tweets' column has some null values to drop and zero dublicated tweets.","metadata":{}},{"cell_type":"code","source":"df.dropna(inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-03-13T14:49:13.527415Z","iopub.execute_input":"2022-03-13T14:49:13.527722Z","iopub.status.idle":"2022-03-13T14:49:13.668810Z","shell.execute_reply.started":"2022-03-13T14:49:13.527681Z","shell.execute_reply":"2022-03-13T14:49:13.667922Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"Let's translate` dialect` column in arabic to an `arabic_dialects` column for some visualization plots.","metadata":{}},{"cell_type":"code","source":"arabic_dialects = {\n    'AE': 'لهجة اماراتية',\n    'BH': 'لهجة بحرينية',\n    'DZ': 'لهجة جزائرية',\n    'EG': 'لهجة مصرية',\n    'IQ': 'لهجة عراقية',\n    'JO': 'لهجة أردنية',\n    'KW': 'لهجة كويتية',\n    'LB': 'لهجة لبنانية',\n    'LY': 'لهجة ليبية',\n    'MA': 'لهجة مغربية',\n    'OM': 'لهجة عمانية',\n    'PL': 'لهجة فلسطينية',\n    'QA': 'لهجة قطرية',\n    'SA': 'لهجة سعودية',\n    'SD': 'لهجة سودانية',\n    'SY': 'لهجة سورية',\n    'TN': 'لهجة تونسية',\n    'YE': 'لهجة يمنية'\n}\ndf['dialect_in_arabic'] = df.dialect.map(arabic_dialects)","metadata":{"execution":{"iopub.status.busy":"2022-03-13T14:49:13.670332Z","iopub.execute_input":"2022-03-13T14:49:13.670919Z","iopub.status.idle":"2022-03-13T14:49:13.738735Z","shell.execute_reply.started":"2022-03-13T14:49:13.670873Z","shell.execute_reply":"2022-03-13T14:49:13.737888Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"Here we create a pie plot to show the percentage of each dialect. This plot shows that the largest percentage for egyptian dialects with 12.6% and the smallest is tunisian dialect with 2.02%.","metadata":{}},{"cell_type":"code","source":"import plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nfrom plotly.subplots import make_subplots\n\n\nvalues = df.dialect_in_arabic.value_counts();\nlabels = values.index\n\nfig = go.Figure(\n    [\n        go.Pie(\n            labels = labels,\n            values = values\n        )\n    ]\n)\nfig.update_layout(title_text=\"Pie chart of arabic dialects\", template=\"plotly_white\")\nfig.data[0].marker.colors = [px.colors.qualitative.Plotly[2:]]\nfig.data[0].textfont.color = \"black\"\nfig.data[0].textposition = \"inside\"\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-13T14:49:13.740385Z","iopub.execute_input":"2022-03-13T14:49:13.740694Z","iopub.status.idle":"2022-03-13T14:49:16.672872Z","shell.execute_reply.started":"2022-03-13T14:49:13.740634Z","shell.execute_reply":"2022-03-13T14:49:16.672082Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"Here to visualize top tokens for each dialect we use 'CountVectorizer'.\n\n'CountVectorizer' is a function to convert a collection of text documents to a matrix of token counts.","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n\ndef get_top_tweet_bigrams(df, column_name, dialect, n=None):\n    # select all text for selected dialect\n    txt = df[df['dialect']==dialect][column_name].str.lower()\n    # create a matrix of token counts\n    vec = CountVectorizer(ngram_range=(2, 2)).fit(txt)\n    # bag of words  \n    bag_of_words = vec.transform(txt)\n    # sum bag of words \n    sum_words = bag_of_words.sum(axis=0) \n    # frequency of each word in selected texts\n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    # top n words repeates in selected texts \n    top = words_freq[:n]\n    #print(top)\n    x, y = map( list, zip(*top) )\n    nn = np.arange(len(x))\n    # reshape arabic words\n    x = [ get_display( arabic_reshaper.reshape(i)) for i in x]\n    \n    plt.figure(figsize=(10,8))\n    plt.barh(x, y, align='center', alpha=0.2)\n    plt.plot(y, nn, '-o', markersize=5, alpha=0.8)\n    plt.yticks(nn, x);\n    plt.xlabel('Word Number');\n    plt.title(f'Top {n} words in {dialect} tweets')","metadata":{"execution":{"iopub.status.busy":"2022-03-13T14:49:16.675438Z","iopub.execute_input":"2022-03-13T14:49:16.675691Z","iopub.status.idle":"2022-03-13T14:49:16.812472Z","shell.execute_reply.started":"2022-03-13T14:49:16.675647Z","shell.execute_reply":"2022-03-13T14:49:16.811684Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"get_top_tweet_bigrams(df, 'tweets', dialect='EG', n=10)\nget_top_tweet_bigrams(df, 'tweets', dialect='TN', n=10)\n#for dialect in df.dialect.unique():\n#    get_top_tweet_bigrams(df, 'tweets', dialect=dialect, n=10)","metadata":{"execution":{"iopub.status.busy":"2022-03-13T14:49:16.813563Z","iopub.execute_input":"2022-03-13T14:49:16.813806Z","iopub.status.idle":"2022-03-13T14:49:27.006912Z","shell.execute_reply.started":"2022-03-13T14:49:16.813778Z","shell.execute_reply":"2022-03-13T14:49:27.006041Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"###### ","metadata":{}},{"cell_type":"markdown","source":"### `Clean Text`\nNext step to try several methods **one at a time** to clean our tweets like:-\n\n        * Select arabic characters only from text\n        * Remove username \"@handle\" from text\n        * Remove URL from text\n        * Remove punctuation, emoji and smileys from text\n        * Remove \\n, \\t ,,, etc from text\n        * Remove Diacritization from text\n        * Remove Arabic Stop Words from text","metadata":{}},{"cell_type":"markdown","source":"#### `Select arabic characters only from text`","metadata":{}},{"cell_type":"code","source":"ar_pattern = r'[\\u0600-\\u06FF]+'\ntest_text = df.tweets.iloc[254]\ntest_text","metadata":{"execution":{"iopub.status.busy":"2022-03-13T14:49:27.008598Z","iopub.execute_input":"2022-03-13T14:49:27.009167Z","iopub.status.idle":"2022-03-13T14:49:27.016340Z","shell.execute_reply.started":"2022-03-13T14:49:27.009116Z","shell.execute_reply":"2022-03-13T14:49:27.015493Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"' '.join(re.findall(ar_pattern, test_text)) # return arabic tokens only","metadata":{"execution":{"iopub.status.busy":"2022-03-13T14:49:27.017920Z","iopub.execute_input":"2022-03-13T14:49:27.018213Z","iopub.status.idle":"2022-03-13T14:49:27.032229Z","shell.execute_reply.started":"2022-03-13T14:49:27.018173Z","shell.execute_reply":"2022-03-13T14:49:27.031615Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"re.sub(ar_pattern, ' ', test_text) # return non arabic tokens","metadata":{"execution":{"iopub.status.busy":"2022-03-13T14:49:27.033457Z","iopub.execute_input":"2022-03-13T14:49:27.033854Z","iopub.status.idle":"2022-03-13T14:49:27.042974Z","shell.execute_reply.started":"2022-03-13T14:49:27.033805Z","shell.execute_reply":"2022-03-13T14:49:27.042400Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"#### `Remove username \"@handle\"`","metadata":{}},{"cell_type":"code","source":"handle_pattern = r'(@.*?)[\\s]'","metadata":{"execution":{"iopub.status.busy":"2022-03-13T14:49:27.044359Z","iopub.execute_input":"2022-03-13T14:49:27.044850Z","iopub.status.idle":"2022-03-13T14:49:27.050551Z","shell.execute_reply.started":"2022-03-13T14:49:27.044808Z","shell.execute_reply":"2022-03-13T14:49:27.049808Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"re.sub(handle_pattern, ' ', test_text) # return text without @handle","metadata":{"execution":{"iopub.status.busy":"2022-03-13T14:49:27.051721Z","iopub.execute_input":"2022-03-13T14:49:27.052228Z","iopub.status.idle":"2022-03-13T14:49:27.061102Z","shell.execute_reply.started":"2022-03-13T14:49:27.052195Z","shell.execute_reply":"2022-03-13T14:49:27.060364Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"#### `Remove URL`","metadata":{}},{"cell_type":"code","source":"url_pattern = r'https?://\\S+|www\\.\\S+'\ntest_text = df.tweets.iloc[1851]\ntest_text","metadata":{"execution":{"iopub.status.busy":"2022-03-13T14:49:27.061958Z","iopub.execute_input":"2022-03-13T14:49:27.062533Z","iopub.status.idle":"2022-03-13T14:49:27.073412Z","shell.execute_reply.started":"2022-03-13T14:49:27.062500Z","shell.execute_reply":"2022-03-13T14:49:27.072649Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"re.findall(url_pattern, test_text) # return url only","metadata":{"execution":{"iopub.status.busy":"2022-03-13T14:49:27.074481Z","iopub.execute_input":"2022-03-13T14:49:27.074936Z","iopub.status.idle":"2022-03-13T14:49:27.081536Z","shell.execute_reply.started":"2022-03-13T14:49:27.074908Z","shell.execute_reply":"2022-03-13T14:49:27.080704Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"re.compile(r'https?://\\S+|www\\.\\S+').sub(r'', test_text) # return text without url","metadata":{"execution":{"iopub.status.busy":"2022-03-13T14:49:27.082645Z","iopub.execute_input":"2022-03-13T14:49:27.083358Z","iopub.status.idle":"2022-03-13T14:49:27.091394Z","shell.execute_reply.started":"2022-03-13T14:49:27.083325Z","shell.execute_reply":"2022-03-13T14:49:27.090851Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"#### `Remove punctuation, emoji and smileys`","metadata":{}},{"cell_type":"code","source":"url_pattern = r'[^\\w\\s]'\ntest_text = df.tweets.iloc[0]\ntest_text","metadata":{"execution":{"iopub.status.busy":"2022-03-13T14:49:27.092268Z","iopub.execute_input":"2022-03-13T14:49:27.092727Z","iopub.status.idle":"2022-03-13T14:49:27.102685Z","shell.execute_reply.started":"2022-03-13T14:49:27.092697Z","shell.execute_reply":"2022-03-13T14:49:27.101852Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"re.findall(url_pattern, test_text)","metadata":{"execution":{"iopub.status.busy":"2022-03-13T14:49:27.103959Z","iopub.execute_input":"2022-03-13T14:49:27.104351Z","iopub.status.idle":"2022-03-13T14:49:27.113923Z","shell.execute_reply.started":"2022-03-13T14:49:27.104312Z","shell.execute_reply":"2022-03-13T14:49:27.113013Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"> this do the same as remove emojy. plus it removes punctuation and smily faces","metadata":{}},{"cell_type":"code","source":"re.sub(url_pattern, '', test_text)","metadata":{"execution":{"iopub.status.busy":"2022-03-13T14:49:27.115201Z","iopub.execute_input":"2022-03-13T14:49:27.115474Z","iopub.status.idle":"2022-03-13T14:49:27.126482Z","shell.execute_reply.started":"2022-03-13T14:49:27.115447Z","shell.execute_reply":"2022-03-13T14:49:27.125710Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"#### `Remove Escape Codes` \n> Escape Codes like \\n, \\t ,,, etc","metadata":{}},{"cell_type":"code","source":"url_pattern = r'\\s+'\ntest_text = df.tweets.iloc[12437]\ntest_text","metadata":{"execution":{"iopub.status.busy":"2022-03-13T14:49:27.130296Z","iopub.execute_input":"2022-03-13T14:49:27.130763Z","iopub.status.idle":"2022-03-13T14:49:27.141328Z","shell.execute_reply.started":"2022-03-13T14:49:27.130715Z","shell.execute_reply":"2022-03-13T14:49:27.140369Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"re.findall(url_pattern,  test_text) # return escape codes found in text","metadata":{"execution":{"iopub.status.busy":"2022-03-13T14:49:27.143040Z","iopub.execute_input":"2022-03-13T14:49:27.143850Z","iopub.status.idle":"2022-03-13T14:49:27.151238Z","shell.execute_reply.started":"2022-03-13T14:49:27.143814Z","shell.execute_reply":"2022-03-13T14:49:27.150537Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"re.sub(url_pattern, ' ', test_text).strip() # return text escape codes found in","metadata":{"execution":{"iopub.status.busy":"2022-03-13T14:49:27.152132Z","iopub.execute_input":"2022-03-13T14:49:27.152721Z","iopub.status.idle":"2022-03-13T14:49:27.159998Z","shell.execute_reply.started":"2022-03-13T14:49:27.152661Z","shell.execute_reply":"2022-03-13T14:49:27.159247Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"#### `Remove Diacritization` (tashkeel)","metadata":{}},{"cell_type":"code","source":"url_pattern = r'[\\u0617-\\u061A\\u064B-\\u0652]'\ntest_text = df.tweets.iloc[12437]\ntest_text","metadata":{"execution":{"iopub.status.busy":"2022-03-13T14:49:27.161084Z","iopub.execute_input":"2022-03-13T14:49:27.161488Z","iopub.status.idle":"2022-03-13T14:49:27.169698Z","shell.execute_reply.started":"2022-03-13T14:49:27.161459Z","shell.execute_reply":"2022-03-13T14:49:27.168925Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"re.findall(url_pattern,  test_text)","metadata":{"execution":{"iopub.status.busy":"2022-03-13T14:49:27.171084Z","iopub.execute_input":"2022-03-13T14:49:27.171629Z","iopub.status.idle":"2022-03-13T14:49:27.180069Z","shell.execute_reply.started":"2022-03-13T14:49:27.171585Z","shell.execute_reply":"2022-03-13T14:49:27.179454Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"re.sub(re.compile(url_pattern),\"\", test_text) # p_tashkeel","metadata":{"execution":{"iopub.status.busy":"2022-03-13T14:49:27.185100Z","iopub.execute_input":"2022-03-13T14:49:27.185344Z","iopub.status.idle":"2022-03-13T14:49:27.192509Z","shell.execute_reply.started":"2022-03-13T14:49:27.185316Z","shell.execute_reply":"2022-03-13T14:49:27.191887Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"#### `Remove Arabic Stop Words`","metadata":{}},{"cell_type":"code","source":"from nltk.corpus import stopwords\nnltk_stopwords = stopwords.words('arabic')\nnltk_stopwords[:10]","metadata":{"execution":{"iopub.status.busy":"2022-03-13T14:49:27.193600Z","iopub.execute_input":"2022-03-13T14:49:27.194422Z","iopub.status.idle":"2022-03-13T14:49:27.716627Z","shell.execute_reply.started":"2022-03-13T14:49:27.194376Z","shell.execute_reply":"2022-03-13T14:49:27.715832Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"import urllib.request  # the lib that handles the url stuff\ntarget_url = \"https://raw.githubusercontent.com/stopwords-iso/stopwords-ar/master/stopwords-ar.txt\"\n\nstopwords_ar = []\nfor line in urllib.request.urlopen(target_url):\n    #print(line.decode('utf-8'))\n    line = line.decode('utf-8').split(\"\\n\")[0]\n    stopwords_ar.append(line)\n    \nstopwords_ar[:10]","metadata":{"execution":{"iopub.status.busy":"2022-03-13T14:49:27.718341Z","iopub.execute_input":"2022-03-13T14:49:27.718904Z","iopub.status.idle":"2022-03-13T14:49:27.890530Z","shell.execute_reply.started":"2022-03-13T14:49:27.718860Z","shell.execute_reply":"2022-03-13T14:49:27.889736Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"test_text = df.tweets.iloc[12437]\n\ndef remove_stop_words(text, stopwords):\n    words = [word for word in text.split() if word not in stopwords]\n    return \" \".join(words)    \n\nprint(remove_stop_words(test_text, stopwords=stopwords_ar))\nprint(remove_stop_words(test_text, stopwords=nltk_stopwords))","metadata":{"execution":{"iopub.status.busy":"2022-03-13T14:49:27.891734Z","iopub.execute_input":"2022-03-13T14:49:27.891970Z","iopub.status.idle":"2022-03-13T14:49:27.898622Z","shell.execute_reply.started":"2022-03-13T14:49:27.891941Z","shell.execute_reply":"2022-03-13T14:49:27.897985Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":"#### Make a Preprocessing Pipleline from all above methods to try which of them make a positive differnce in score.","metadata":{}},{"cell_type":"code","source":"def clean_text(text):\n    text = re.sub(r'(@.*?)[\\s]', ' ', text)\n    text = re.compile(r'https?://\\S+|www\\.\\S+').sub(r'', text)\n    text = re.sub(r'[^\\w\\s]', '', text)\n    text = re.sub(r'\\s+', ' ', text).strip()\n    text = re.sub(re.compile(r'[\\u0617-\\u061A\\u064B-\\u0652]'),\"\", text)\n    text = remove_stop_words(text, stopwords_ar)\n    text = text.strip()\n    return text\n\ndf['clean_tweets'] = df.tweets.apply(lambda t: clean_text(t))","metadata":{"execution":{"iopub.status.busy":"2022-03-13T14:49:27.899871Z","iopub.execute_input":"2022-03-13T14:49:27.900375Z","iopub.status.idle":"2022-03-13T14:50:17.814244Z","shell.execute_reply.started":"2022-03-13T14:49:27.900324Z","shell.execute_reply":"2022-03-13T14:50:17.813354Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"Let's visualize top 20 tokens after preprocessing pipline.","metadata":{}},{"cell_type":"code","source":"get_top_tweet_bigrams(df, 'clean_tweets', dialect='EG', n=20)\nget_top_tweet_bigrams(df, 'clean_tweets', dialect='LY', n=20)\n#for dialect in df.dialect.unique():\n#    get_top_tweet_bigrams(df, 'clean_tweets', dialect=dialect, n=20)","metadata":{"execution":{"iopub.status.busy":"2022-03-13T14:50:17.815370Z","iopub.execute_input":"2022-03-13T14:50:17.815574Z","iopub.status.idle":"2022-03-13T14:50:27.814042Z","shell.execute_reply.started":"2022-03-13T14:50:17.815548Z","shell.execute_reply":"2022-03-13T14:50:27.813523Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":"LabelEncoder to encode target labels with value between 0 and n_classes-1 (0 to 17)","metadata":{}},{"cell_type":"code","source":"#from sklearn.preprocessing import OneHotEncoder \nfrom sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\ndf['dialect_encode'] = le.fit_transform( df['dialect'] ).astype(np.int8)\ntarget_names = le.inverse_transform(np.arange(18))\ntarget_names","metadata":{"execution":{"iopub.status.busy":"2022-03-13T14:50:27.815051Z","iopub.execute_input":"2022-03-13T14:50:27.815530Z","iopub.status.idle":"2022-03-13T14:50:27.949451Z","shell.execute_reply.started":"2022-03-13T14:50:27.815500Z","shell.execute_reply":"2022-03-13T14:50:27.948749Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":"![](https://miro.medium.com/max/1200/1*3-KkZ0hlRZxjMn7Z6uXDGg.png)","metadata":{}},{"cell_type":"markdown","source":"# Major regions for dialects\nAfter run several models we notice that Most of misclassifications happens for each dialect with dialects for share border countries. So we can break a classifier into three to five classifiers. each classifier for a group of share border countries.\n\nHere we can Combine dialects with dialects for share border countries. So let's take a small look here.","metadata":{}},{"cell_type":"markdown","source":"Starting with define our new regions for dialects","metadata":{}},{"cell_type":"code","source":"new_regions = {\n    'ar_island': ['QA', 'SA', 'AE', 'KW', 'OM', 'YE','BH'],\n    'shaam': ['LB', 'SY', 'JO', 'PL', 'IQ'],\n    'africa_ne': ['EG', 'LY', 'SD'],\n    'africa_nw': ['TN', 'MA', 'DZ'],\n}\nnew_regions_names = new_regions.keys()\n# prepare last dictionary for pandas map function to easly store in dataset\nnew_regions = {val:key for key, lst in new_regions.items() for val in lst}\n#new_regions","metadata":{"execution":{"iopub.status.busy":"2022-03-13T15:10:44.809301Z","iopub.execute_input":"2022-03-13T15:10:44.809582Z","iopub.status.idle":"2022-03-13T15:10:44.818372Z","shell.execute_reply.started":"2022-03-13T15:10:44.809548Z","shell.execute_reply":"2022-03-13T15:10:44.817613Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"markdown","source":"Create a function to combine deli","metadata":{}},{"cell_type":"code","source":"new_df = df.copy()\nnew_df['region'] = new_df['dialect'].map(new_regions)\nnew_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-13T14:57:21.667366Z","iopub.execute_input":"2022-03-13T14:57:21.668089Z","iopub.status.idle":"2022-03-13T14:57:21.757521Z","shell.execute_reply.started":"2022-03-13T14:57:21.668040Z","shell.execute_reply":"2022-03-13T14:57:21.756625Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"new_df.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-03-13T14:57:25.172446Z","iopub.execute_input":"2022-03-13T14:57:25.172778Z","iopub.status.idle":"2022-03-13T14:57:25.440165Z","shell.execute_reply.started":"2022-03-13T14:57:25.172743Z","shell.execute_reply":"2022-03-13T14:57:25.439210Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import precision_score, classification_report, confusion_matrix\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import LinearSVC\n\nX = TfidfVectorizer().fit_transform(new_df['tweets'])\ny = new_df.region\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.7, shuffle=True, random_state = 42)\n\nmodel = LinearSVC()\nmodel.fit(X_train, y_train)\npredicted = model.predict(X_val)\nprecision_score(predicted, y_val, average='micro')","metadata":{"execution":{"iopub.status.busy":"2022-03-13T15:06:00.050938Z","iopub.execute_input":"2022-03-13T15:06:00.051639Z","iopub.status.idle":"2022-03-13T15:06:23.700434Z","shell.execute_reply.started":"2022-03-13T15:06:00.051600Z","shell.execute_reply":"2022-03-13T15:06:23.699680Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"def ConfusionMatrix(cm, target_names):\n    #cm = confusion_matrix(y_val, predicted)\n    df_cm = pd.DataFrame(cm, target_names, target_names)\n\n    plt.figure(figsize = (10, 10))\n    sns.set(font_scale=1.4) # for label size\n    sns.heatmap(\n        df_cm,\n        fmt=\".1f\", annot=True, annot_kws={'size': 12}, cmap=cmap,\n        linewidths=3,\n    );\n    return None\n\nConfusionMatrix(confusion_matrix(y_val, predicted), new_regions_names)","metadata":{"execution":{"iopub.status.busy":"2022-03-13T15:11:21.295952Z","iopub.execute_input":"2022-03-13T15:11:21.296214Z","iopub.status.idle":"2022-03-13T15:11:23.846033Z","shell.execute_reply.started":"2022-03-13T15:11:21.296185Z","shell.execute_reply":"2022-03-13T15:11:23.845221Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"markdown","source":"> We can try to reallocate dialects in diffrent regions for better score.","metadata":{}},{"cell_type":"markdown","source":"> ## `Furthermore we can break a classifier into three to five classifiers to classiffy dialects in the same regions.`","metadata":{}},{"cell_type":"markdown","source":"![](https://miro.medium.com/max/1200/1*3-KkZ0hlRZxjMn7Z6uXDGg.png)","metadata":{}},{"cell_type":"code","source":"print(\"let's stop here for now\")\nbreak;","metadata":{"execution":{"iopub.status.busy":"2022-03-13T14:50:28.333220Z","iopub.status.idle":"2022-03-13T14:50:28.333555Z","shell.execute_reply.started":"2022-03-13T14:50:28.333390Z","shell.execute_reply":"2022-03-13T14:50:28.333410Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### t-SNE Corpus Visualization\n        One very popular method for visualizing document similarity is to use t-distributed stochastic neighbor embedding. By decomposing high-dimensional document vectors into 2 dimensions using probability distributions from both the original dimensionality and the decomposed dimensionality, t-SNE is able to effectively cluster similar documents. By decomposing to 2 or 3 dimensions, the documents can be visualized with a scatter plot.","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import PCA, TruncatedSVD\nfrom sklearn.manifold import TSNE\nfrom yellowbrick.text import TSNEVisualizer","metadata":{"execution":{"iopub.status.busy":"2022-03-13T14:50:28.335014Z","iopub.status.idle":"2022-03-13T14:50:28.335345Z","shell.execute_reply.started":"2022-03-13T14:50:28.335171Z","shell.execute_reply":"2022-03-13T14:50:28.335203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Vectorized_data = TfidfVectorizer().fit_transform(df['clean_tweets'])\ny = df.dialect\n\n# Create the visualizer and draw the vectors\ntsne = TSNEVisualizer()\ntsne.fit(Vectorized_data, y)\ntsne.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-13T14:50:28.336146Z","iopub.status.idle":"2022-03-13T14:50:28.336988Z","shell.execute_reply.started":"2022-03-13T14:50:28.336779Z","shell.execute_reply":"2022-03-13T14:50:28.336803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note that you can pass the class labels or document categories directly to the TSNEVisualizer as follows:","metadata":{}},{"cell_type":"code","source":"tsne = TSNEVisualizer(labels=labels)\ntsne.fit(X, y)\ntsne.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-13T14:50:28.338218Z","iopub.status.idle":"2022-03-13T14:50:28.338890Z","shell.execute_reply.started":"2022-03-13T14:50:28.338677Z","shell.execute_reply":"2022-03-13T14:50:28.338703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}